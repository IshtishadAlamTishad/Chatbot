{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614262ff",
   "metadata": {},
   "source": [
    "### developing chatbot plan : \n",
    "first i will extract the data from .pdf by mannual...\n",
    "then i will tokenize the each words.\n",
    "\n",
    "the model will be simple llm model . i will use \"Attention is what you need\" paper's diagram to create the model\n",
    "from scratch.. \n",
    "\n",
    "for the model developement : \n",
    "\n",
    "first i will create the :  \n",
    "1) multihead attention (Query,Key,Value) function \n",
    "2) text embedding function\n",
    "3) positional encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from module.createData import CreateDataset\n",
    "from module.readData import ReadDataset\n",
    "from module.cuda import getDevice\n",
    "from module.bot import Chatbot\n",
    "from module.Transformer import Transformer\n",
    "from module.TrainModel import trainModel\n",
    "from module.TestModel import Test\n",
    "\n",
    "from module.VD import VectorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10be0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('asset/data/data.txt', 'r',encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c89539db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1.2 \\nFINANCIAL POLICY OBJECTIVES AND STRATEGIES \\n STATEMENT \\nThe presentation and preparation of the Territory’s Budget is provided for in sections 11 and \\n11A of the Financial Management Act 1996 (the Act).   \\nThe purpose of the financial policy objectives and strategies statement is to make transparent \\nthe Government’s financial strategies and to establish a benchmark for evaluating the \\nGovernment’s conduct of financial policy.  The statement is also consistent with section \\n11(1)(a) of the Act. \\nStrategic Priorities and Financial Policy \\nIn this budget, the Government continues its commitment to the principles of responsible \\nfinancial management. \\nThe financial objectives and the key measures below outline the Government’s financial \\npolicy.  Strategic priorities, as they relate to the Territory’s Budget, are summarised as: \\n• maintain a balanced budget over the economic cycle; \\n• maintain low levels of debt; \\n• provide the highest possible standard of government services; \\n• service delivery which focuses on people, the environment and building prosperity;  \\n• maintain a triple A credit rating; and \\n• effective integration of economic and environmental considerations to promote \\nsustainability of service delivery. \\nThe 2005-06 Budget and Forward Estimates have been prepared taking into account the need \\nto provide sustainable social and economic services and infrastructure to the community and \\nthe objective of ecologically sustainable development, as required by the Act.  Future \\nBudgets will over time, as these principles become more firmly integrated with decision \\nmaking processes, demonstrate stronger linkages with these principles.   \\nThe Budget is economically sustainable.  The Budget provides an aggregate surplus over four \\nyears.  Short and long-term financial indicators are sound.  Cash reserves are reducing \\nthrough to 2006-07 but are forecast to grow in 2008-09, and notably, general government \\nborrowings have not increased.   \\nThe Budget is socially responsible.  Considerable resources are directed into key service \\ndelivery areas, such as health, education, child and family support.  In many cases, these \\nresources are dedicated to meeting the increasing demand of a growing and ageing \\npopulation; for example, hospital waiting lists, dental services, and provision of traineeships \\nand apprenticeships to meet skill shortages.  In other cases, resources are targeted at those \\nmost vulnerable and in need; for example, provision for children with disability, family \\nviolence intervention and complex needs, child protection, and pensioner concessions.   \\n2005-06 Budget Paper No. 3 \\n5 \\nFinancial Policy Objectives and Strategies Statement \\nFinally, the Budget supports community welfare and development.  Investments are made \\nwith both short and long-term objectives in mind.  Various projects associated with \\nimplementing the Shaping Our Territory plan and ACT Energy Wise Program are provided \\nfor, while the Asbestos Assessment Task Force helps communicate the risk of asbestos and \\neducates to raise confidence in managing asbestos issues in the community. \\nFinancial Objectives and Key Measures \\nThe following table provides the short-term and long-term financial objectives of the \\nGovernment. \\nTable 1.2.1 \\nFinancial Objectives and Measures \\nShort Term Financial Objectives \\nLong Term Financial Objectives \\nMaintain a Triple A credit rating \\nMaintain low levels of debt \\nMaintain Territory infrastructure \\nMake adequate provision for long-term liabilities \\nStrategic approach to the capital works program Minimise risk to Territory finances and assets \\nProvision of the highest possible standard of \\nGovernment services, and maintain service levels, \\nhaving regard to growth and monetary inflation \\nKey Measures \\nKey Measures \\nMaintain a balanced budget over the economic cycle \\nof the current budget estimates from 2005-06 to \\n2008-09 \\n90% coverage of accrued superannuation liabilities by \\n2039-2040 \\nMaintain the capital infrastructure of the Territory Maintain or reduce GGS debt \\nMaintain net interest cost as a proportion of total \\nown-source revenue for the GGS less than zero \\nAdequate systems and processes in place to recognise \\nand mitigate risk \\nMaintain levels of taxation as a proportion of GSP  \\nMaintain or increase the Territory’s Net Assets  \\nMaintain a Balanced Budget over the economic cycle \\nThe balanced budget financial objective is measured by the operating result of the General \\nGovernment Sector (GGS).  A four-year planning horizon recognises that it is not always \\nnecessary to deliver surpluses every year without any regard to the broader context of the \\neconomy and the service and infrastructure requirements of the community.  What is \\nimportant is that a surplus is delivered over the economic cycle. \\n2005-06 Budget Paper No. 3 \\n6 \\nFinancial Policy Objectives and Strategies Statement \\nThe Government’s forecast operating result is presented in the following table.   \\nTable 1.2.2 \\nGeneral Government Sector \\n2005-06 Budget and Forward Estimates \\n2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09\\n Actual Actual Est. Out. Budget Estimate Estimate Estimate\\n  \\n$m $m $m $m $m $m $m\\n \\nGGS Operating Result   154.6  70.5  52.2-  91.5  0.9  39.3  73.3\\n Aggregate Result 2002-03 to \\n2005-06 \\n  185.7\\n Aggregate Result 2005-06 to \\n2008-09 \\n22.0\\n         \\n \\nWhile the 2005-06 Budget is in deficit, the Government has introduced significant measures \\nincluded to return the Budget to surplus, such as savings measures aimed at improved \\nefficiency. \\nMaintain the Capital Infrastructure of the Territory \\nThe capital infrastructure objective of the Territory is measured through the value of current \\nand non-current works in progress, and the value of property, plant and equipment in the \\nBalance Sheet of the Territory. \\nThe Government’s target for this measure is to demonstrate and provide for the maintenance \\nof the existing level of physical assets (as at 30 June 2005). \\nTable 1.2.3 \\nGeneral Government Sector \\nCapital Infrastructure \\n2004-05 2005-06 2006-07 2007-08 2008-09\\n Est. Out. Budget Estimate Estimate Estimate\\n  \\n$m $m $m $m $m\\n  \\nNon-Current Capital Works in Progress   136  218  263  63   41\\n Property, Plant and Equipment 9  905 10  205 10  437 10  847 11  034\\n Sub Total 10  040 10  423 10  701 10  910 11  075\\n            \\n \\nMaintain net interest cost as a proportion of total own-source revenue for the General \\nGovernment Sector less than zero \\nNet interest is the difference between interest earned on investments and interest paid on \\ndebt.  Total own-source revenue is the difference between Total Revenue and \\nCommonwealth Grants in the Operating Statement.  \\n2005-06 Budget Paper No. 3 7 Financial Policy Objectives and Strategies Statement \\nThis objective provides an indication of the Government’s ability to meet its debt obligations \\nwithout impacting on services, and in the current case for the General Government Sector, a \\nnet interest return for the Territory.  The Government’s target for this measure is to ensure \\nthat the level of net interest cost remains negative, indicating the Territory can comfortably \\nmeet interest expenses. \\nTable 1.2.4 \\nGeneral Government Sector \\nNet Interest Expense as a percentage of Own-Source Revenue \\n2004-05 2005-06 2006-07 2007-08 2008-09\\n Est. Out. Budget Estimate Estimate Estimate\\n  \\n$m $m $m $m $m\\n  \\nInterest Expense   47  49  49  52   54\\n Interest Revenue   78  69  58  54   55\\n  \\nNet Interest Cost -  31-  20-  8-  2 -  2\\n  \\nOwn Source Revenue 1  617 1  568 1  669 1  763 1  857\\n  \\nNet Interest Cost as a % of Own Source \\nRevenue -1.9%-1.3%-0.5%-0.1% -0.1%\\n            \\n \\nMaintain levels of taxation as a proportion of GSP \\nThe objective of maintaining taxation levels as a proportion of GSP (Gross State Product) \\nensures that levels of tax burden on the community do not increase disproportionally to the \\nlevel of economic activity of the Territory. \\nTable 1.2.5 \\nGeneral Government Sector \\nProportion of Taxation Revenue to Gross State Product \\n2004-05 2005-06 2006-07 2007-08 2008-09\\n Est. Out. Budget Estimate Estimate Estimate\\n  \\n$m $m $m $m $m\\n  \\nTaxation     692   729   778   833    889\\n GSP  16  944 17  637 18  349 19  240  20  214\\n  \\nTaxation as a % of GSP 4.1% 4.1% 4.2% 4.3% 4.4%\\n            \\n \\nMaintain or increase the Territory’s Net Assets \\nThis objective’s aim is to budget for an increase (or maintenance) of the level of net assets of \\nthe Territory.  This represents the difference between Total Assets and Total Liabilities \\nrepresented on the Balance Sheet. \\n2005-06 Budget Paper No. 3 8 Financial Policy Objectives and Strategies Statement \\n \\nTable 1.2.6 \\nTotal Territory \\nMaintenance of Net Assets \\n2004-05 2005-06 2006-07 2007-08 2008-09\\n Est. Out. Budget Estimate Estimate Estimate\\n  \\n$m $m $m $m $m\\n  \\nTotal Assets  13  340 13  637 14  025 14  446  14  905\\n Total Liabilities  4  036 4  302 4  555 4  789  5  027\\n  \\nTotal Territory Net Assets  9  304 9  335 9  470 9  657  9  878\\n            \\n \\n90% coverage of accrued superannuation liabilities by 2039-40 \\nThis objective is measured by the extent to which the accrued and projected superannuation \\nliability is funded.  The Government has a commitment to fund 90% of accrued \\nsuperannuation liabilities by 30 June 2040.  In light of the significantly lower projected \\nliabilities following the introduction of the PSS accumulation scheme, this target date will be \\nreviewed during 2005-06 with a view to determining whether an earlier date should be \\nadopted.  The outcome of this process will be reported in the 2006-07 Budget. \\nTable 1.2.7 \\nPercentage funding of Superannuation Liabilities \\nAssets Liabilities % Funded\\n 30 June $'000 $'000\\n 2005            1  447  094           2  480  943 58%\\n 2006            1  626  868           2  707  023 60%\\n 2007            1  829  509            2  927  773 62%\\n 2008            2  042  190           3  146  890 65%\\n 2009 2  266  537 3  365  107 67%\\n \\n \\nPrinciples of Responsible Financial Management \\nThe key financial measures established by the Government satisfy various principles of \\nresponsible financial management specified within the Financial Management Act 1996, \\nthese are: \\n(a) ensuring that the total liabilities of the Territory are at prudent levels to provide a buffer \\nagainst factors that may impact adversely on the level of total Territory liabilities in the \\nfuture, and ensuring that, until prudent levels have been achieved, the total operating \\nexpenses of the Territory in each financial year are less than its operating income levels \\nin the same financial year; \\n(b) when prudent levels of total Territory liabilities have been achieved, maintaining the \\nlevels by ensuring that, on average, over a reasonable period of time, the total operating \\nexpenses of the Territory do not exceed its operating income levels;  \\n2005-06 Budget Paper No. 3 9 Financial Policy Objectives and Strategies Statement \\n(c) \\nachieving and maintaining levels of Territory net worth to provide a buffer against \\nfactors that may impact adversely on levels of Territory net worth in the future;  \\n(d) \\n(e) \\n(f) \\nmanaging prudently the fiscal risks of the Territory; \\npursuing spending and taxing policies that are consistent with a reasonable degree of \\nstability and predictability in the level of the tax burden; and \\ngiving full, accurate and timely disclosure of financial information about the activities \\nof the government and its agencies. \\nThe key financial objectives outlined earlier in this chapter support the principles of \\nresponsible fiscal management. \\nThe objectives of a balanced budget over the economic cycle, the maintenance of low levels \\nof debt and adequate provision for long-term liabilities strongly support prudent financial \\nmanagement and the principles of inter-generational equity.   \\nExpenses must be balanced against revenues, borrowings must be within manageable limits \\nand repayments affordable from ongoing revenues.   \\nThe Territory’s prudent investment policies are designed to protect the value of the \\nTerritory’s more liquid assets thus maintaining this asset base for the settlement of future \\nliabilities.   \\nA strategic approach to capital works programs reflects a longer term view of key \\ninfrastructure decisions such as the timely delivery of new infrastructure and the timely \\nreplacement of infrastructure balanced against the changing demographics and changing \\nservice delivery needs. \\nThe 2005-06 Budget contains a significant level of funding for new construction and \\nupgrades of existing infrastructure. \\nA number of strategic new infrastructure items have been approved for construction or design \\nin the 2005-06 Budget to enhance services for the people of the ACT.  These include a major \\nnew recreational area at Stromlo Forest Park, a new Primary and Pre-School in East \\nGungahlin and a replacement for the Quamby Youth Detention Centre.  Additional funding \\nfor continuing projects such as the Gungahlin Drive Extension and the Alexander \\nMaconachie Centre (Correctional Facility) has also been included. \\nA base level of funding has also been provided for capital upgrades; that is, works which \\nincrease service delivery capacity or extend the useful lives of existing assets.  A further \\nprovision has been made to finance capital upgrades through a five-year rolling program. \\n2005-06 Budget Paper No. 3 \\n10 \\nFinancial Policy Objectives and Strategies Statement \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e199d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "getDevice = getDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d371898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractKeyInfo(InputFilePath, OutputFilePath):\n",
    "    if not os.path.exists(InputFilePath):\n",
    "        raise FileNotFoundError(f\"Input file {InputFilePath} not found.\")\n",
    "    \n",
    "    with open(InputFilePath, 'r', encoding='utf-8') as F:\n",
    "        Document = F.read()\n",
    "    \n",
    "    Pages = re.split(r'2005-06 Budget Paper No. 3\\s*(\\d+)\\s*Financial Policy Objectives and Strategies Statement', Document)\n",
    "    PageContents = []\n",
    "    CurrentPage = 5\n",
    "    for I in range(0, len(Pages), 2):\n",
    "        Content = Pages[I].strip()\n",
    "        if Content:\n",
    "            PageContents.append({'Page': CurrentPage, 'Content': Content})\n",
    "        if I+1 < len(Pages):\n",
    "            CurrentPage = int(Pages[I+1])\n",
    "    \n",
    "    KeyInfo = []\n",
    "    Strategic = re.search(r'Strategic priorities, as they relate to the Territory’s Budget, are summarised as:(.*?)\\. The 2005-06 Budget and Forward Estimates', Document, re.S)\n",
    "    if Strategic:\n",
    "        Priorities = re.findall(r'• (.*?);', Strategic.group(1))\n",
    "        for P in Priorities:\n",
    "            KeyInfo.append({'Info': P.strip(), 'Source': f'Page {CurrentPage}'})\n",
    "    \n",
    "    KeyInfo.extend([\n",
    "        {'Info': 'The Budget provides an aggregate surplus over four years. Cash reserves reducing through to 2006-07 but forecast to grow in 2008-09, general government borrowings not increased.', 'Source': 'Page 5'},\n",
    "        {'Info': 'Resources directed into health, education, child and family support, meeting demand of growing and ageing population, targeted at vulnerable.', 'Source': 'Page 5'},\n",
    "        {'Info': 'Investments in Shaping Our Territory plan, ACT Energy Wise Program, Asbestos Assessment Task Force.', 'Source': 'Page 5'},\n",
    "        {'Info': 'Financial Objectives and Measures (Table 1.2.1): Short Term: Maintain Triple A credit rating, low debt, infrastructure, long-term liabilities, strategic capital works, minimise risk, high standard services. Long Term: Balanced budget, 90% superannuation coverage by 2039-2040, reduce GGS debt, net interest cost <0, mitigate risk, taxation % GSP, increase net assets.', 'Source': 'Page 6'},\n",
    "        {'Info': 'GGS Operating Result (Table 1.2.2): 2002-03 154.6m, 2003-04 70.5m, 2004-05 -52.2m, 2005-06 91.5m, 2006-07 0.9m, 2007-08 39.3m, 2008-09 73.3m. Aggregate 2002-03 to 2005-06 185.7m, 2005-06 to 2008-09 22.0m.', 'Source': 'Page 6'},\n",
    "        {'Info': 'Capital Infrastructure (Table 1.2.3): Non-Current WIP 136m to 41m, Property Plant Equipment 9,905m to 11,034m, Sub Total 10,040m to 11,075m.', 'Source': 'Page 6'},\n",
    "        {'Info': 'Net Interest Cost % Own-Source Revenue (Table 1.2.4): -1.9% to -0.1%.', 'Source': 'Page 7'},\n",
    "        {'Info': 'Taxation % GSP (Table 1.2.5): 4.1% to 4.4%.', 'Source': 'Page 8'},\n",
    "        {'Info': 'Total Territory Net Assets (Table 1.2.6): 9,304m to 9,878m.', 'Source': 'Page 8'},\n",
    "        {'Info': 'Superannuation Liabilities Funding (Table 1.2.7): 58% in 2005 to 67% in 2009.', 'Source': 'Page 9'},\n",
    "        {'Info': 'Principles of Responsible Financial Management: Prudent liabilities, balanced expenses, net worth buffer, fiscal risks, stable taxing, full disclosure.', 'Source': 'Page 9'},\n",
    "        {'Info': 'Infrastructure investments: Stromlo Forest Park, Primary and Pre-School in East Gungahlin, Quamby replacement, Gungahlin Drive Extension, Alexander Maconachie Centre.', 'Source': 'Page 10'},\n",
    "    ])\n",
    "    \n",
    "    with open(OutputFilePath, 'w', encoding='utf-8') as F:\n",
    "        json.dump(KeyInfo, F, indent=4)\n",
    "    \n",
    "    return KeyInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "331a7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDatasetFile(KeyInfo, DatasetFilePath):\n",
    "    ansPair = [\n",
    "        (\"What is the budget strategy?\", \"The Budget provides an aggregate surplus over four years. Cash reserves reducing through to 2006-07 but forecast to grow in 2008-09, general government borrowings not increased. (Source: Page 5)\"),\n",
    "        (\"What about debt levels?\", \"Financial Objectives and Measures (Table 1.2.1): Short Term: Maintain Triple A credit rating, low debt, infrastructure, long-term liabilities, strategic capital works, minimise risk, high standard services. Long Term: Balanced budget, 90% superannuation coverage by 2039-2040, reduce GGS debt, net interest cost <0, mitigate risk, taxation % GSP, increase net assets. (Source: Page 6)\"),\n",
    "        (\"What are the infrastructure investments?\", \"Infrastructure investments: Stromlo Forest Park, Primary and Pre-School in East Gungahlin, Quamby replacement, Gungahlin Drive Extension, Alexander Maconachie Centre. (Source: Page 10)\"),\n",
    "        (\"What is the net interest cost?\", \"Net Interest Cost % Own-Source Revenue (Table 1.2.4): -1.9% to -0.1%. (Source: Page 7)\"),\n",
    "        (\"How is taxation managed?\", \"Taxation % GSP (Table 1.2.5): 4.1% to 4.4%. (Source: Page 8)\"),\n",
    "        (\"What about superannuation liabilities?\", \"Superannuation Liabilities Funding (Table 1.2.7): 58% in 2005 to 67% in 2009. (Source: Page 9)\"),\n",
    "        (\"What are the principles of financial management?\", \"Principles of Responsible Financial Management: Prudent liabilities, balanced expenses, net worth buffer, fiscal risks, stable taxing, full disclosure. (Source: Page 9)\"),\n",
    "        (\"What are the strategic priorities?\", \"maintain a balanced budget over the economic cycle (Source: Page 5)\"),\n",
    "        (\"What are the services prioritized?\", \"Resources directed into health, education, child and family support, meeting demand of growing and ageing population, targeted at vulnerable. (Source: Page 5)\"),\n",
    "        (\"What are the investment programs?\", \"Investments in Shaping Our Territory plan, ACT Energy Wise Program, Asbestos Assessment Task Force. (Source: Page 5)\")\n",
    "    ]\n",
    "    \n",
    "    with open(DatasetFilePath, 'w', encoding='utf-8') as F:\n",
    "        for Question, Answer in ansPair:\n",
    "            F.write(f'\"{Question}\", \"{Answer}\"\\n')\n",
    "    \n",
    "    return ansPair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3981c6d",
   "metadata": {},
   "source": [
    "### Datapath initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1214010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputFilePath = 'asset/data/data.txt'\n",
    "JsonFilePath = 'asset/preprocessed/extractedData.json'\n",
    "DatasetFilePath = 'asset/preprocessed/dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ea5b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(JsonFilePath):\n",
    "    KeyInfo = ExtractKeyInfo(InputFilePath, JsonFilePath)\n",
    "else:\n",
    "    with open(JsonFilePath, 'r', encoding='utf-8') as F:\n",
    "        KeyInfo = json.load(F)\n",
    "\n",
    "\n",
    "if not os.path.exists(DatasetFilePath):\n",
    "    GenerateDatasetFile(KeyInfo, DatasetFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8d73a",
   "metadata": {},
   "source": [
    "### Initializing vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ae3f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "Db = VectorDatabase(KeyInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2491f",
   "metadata": {},
   "source": [
    "### vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "417138e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for Item in KeyInfo:\n",
    "    vocab.update(Item['Info'].lower().translate(str.maketrans('', '', string.punctuation)).split())\n",
    "vocab.update(['<START>', '<END>', '<PAD>'])\n",
    "\n",
    "lti = {Word: Idx for Idx, Word in enumerate(vocab)}\n",
    "\n",
    "itl = {Idx: Word for Word, Idx in lti.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93b24abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coverage': 0, 'prudent': 1, 'vulnerable': 2, 'population': 3, 'taxation': 4, 'in': 5, 'into': 6, 'and': 7, 'table': 8, '200304': 9, 'gungahlin': 10, '200405': 11, '9304m': 12, '67': 13, 'through': 14, 'financial': 15, 'a': 16, 'aggregate': 17, 'infrastructure': 18, '522m': 19, 'drive': 20, '733m': 21, '41m': 22, 'years': 23, 'provides': 24, 'force': 25, 'capital': 26, 'grow': 27, 'plan': 28, 'assets': 29, 'operating': 30, 'an': 31, 'family': 32, 'longterm': 33, 'objectives': 34, '11034m': 35, 'primary': 36, 'maconachie': 37, 'result': 38, '200708': 39, 'risks': 40, 'debt': 41, 'assessment': 42, '200809': 43, 'the': 44, 'act': 45, 'resources': 46, '393m': 47, 'worth': 48, 'directed': 49, 'our': 50, 'reserves': 51, 'works': 52, 'preschool': 53, 'of': 54, 'budget': 55, 'cash': 56, 'term': 57, 'funding': 58, 'increase': 59, 'ggs': 60, 'alexander': 61, 'taxing': 62, '<PAD>': 63, 'park': 64, 'strategic': 65, 'but': 66, 'disclosure': 67, '200203': 68, 'rating': 69, 'equipment': 70, 'revenue': 71, 'ownsource': 72, 'task': 73, 'expenses': 74, 'gsp': 75, 'not': 76, '<END>': 77, 'noncurrent': 78, '9905m': 79, 'total': 80, 'replacement': 81, '44': 82, 'net': 83, '11075m': 84, 'four': 85, 'health': 86, '126': 87, 'wip': 88, '124': 89, 'standard': 90, 'surplus': 91, 'general': 92, 'demand': 93, 'education': 94, 'forecast': 95, '122': 96, 'short': 97, '58': 98, '2005': 99, 'growing': 100, 'by': 101, 'principles': 102, 'investments': 103, 'forest': 104, 'reduce': 105, 'triple': 106, 'to': 107, 'asbestos': 108, 'cost': 109, '19': 110, 'interest': 111, '1546m': 112, '200607': 113, 'borrowings': 114, 'energy': 115, 'low': 116, 'mitigate': 117, 'increased': 118, 'fiscal': 119, '123': 120, 'targeted': 121, 'support': 122, 'ageing': 123, 'extension': 124, '0': 125, 'liabilities': 126, '125': 127, 'territory': 128, 'government': 129, 'program': 130, '41': 131, 'wise': 132, 'services': 133, 'plant': 134, 'measures': 135, '01': 136, '2009': 137, '09m': 138, '9878m': 139, 'centre': 140, 'long': 141, 'sub': 142, 'responsible': 143, '136m': 144, 'stable': 145, '705m': 146, '121': 147, '220m': 148, 'meeting': 149, 'shaping': 150, '90': 151, '<START>': 152, 'buffer': 153, 'reducing': 154, 'superannuation': 155, '1857m': 156, '20392040': 157, '10040m': 158, 'maintain': 159, 'quamby': 160, 'minimise': 161, '915m': 162, 'property': 163, 'at': 164, '127': 165, 'full': 166, 'management': 167, 'credit': 168, 'risk': 169, '200506': 170, 'balanced': 171, 'child': 172, 'stromlo': 173, 'east': 174, 'over': 175, 'high': 176}\n"
     ]
    }
   ],
   "source": [
    "print(lti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30be54b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'coverage', 1: 'prudent', 2: 'vulnerable', 3: 'population', 4: 'taxation', 5: 'in', 6: 'into', 7: 'and', 8: 'table', 9: '200304', 10: 'gungahlin', 11: '200405', 12: '9304m', 13: '67', 14: 'through', 15: 'financial', 16: 'a', 17: 'aggregate', 18: 'infrastructure', 19: '522m', 20: 'drive', 21: '733m', 22: '41m', 23: 'years', 24: 'provides', 25: 'force', 26: 'capital', 27: 'grow', 28: 'plan', 29: 'assets', 30: 'operating', 31: 'an', 32: 'family', 33: 'longterm', 34: 'objectives', 35: '11034m', 36: 'primary', 37: 'maconachie', 38: 'result', 39: '200708', 40: 'risks', 41: 'debt', 42: 'assessment', 43: '200809', 44: 'the', 45: 'act', 46: 'resources', 47: '393m', 48: 'worth', 49: 'directed', 50: 'our', 51: 'reserves', 52: 'works', 53: 'preschool', 54: 'of', 55: 'budget', 56: 'cash', 57: 'term', 58: 'funding', 59: 'increase', 60: 'ggs', 61: 'alexander', 62: 'taxing', 63: '<PAD>', 64: 'park', 65: 'strategic', 66: 'but', 67: 'disclosure', 68: '200203', 69: 'rating', 70: 'equipment', 71: 'revenue', 72: 'ownsource', 73: 'task', 74: 'expenses', 75: 'gsp', 76: 'not', 77: '<END>', 78: 'noncurrent', 79: '9905m', 80: 'total', 81: 'replacement', 82: '44', 83: 'net', 84: '11075m', 85: 'four', 86: 'health', 87: '126', 88: 'wip', 89: '124', 90: 'standard', 91: 'surplus', 92: 'general', 93: 'demand', 94: 'education', 95: 'forecast', 96: '122', 97: 'short', 98: '58', 99: '2005', 100: 'growing', 101: 'by', 102: 'principles', 103: 'investments', 104: 'forest', 105: 'reduce', 106: 'triple', 107: 'to', 108: 'asbestos', 109: 'cost', 110: '19', 111: 'interest', 112: '1546m', 113: '200607', 114: 'borrowings', 115: 'energy', 116: 'low', 117: 'mitigate', 118: 'increased', 119: 'fiscal', 120: '123', 121: 'targeted', 122: 'support', 123: 'ageing', 124: 'extension', 125: '0', 126: 'liabilities', 127: '125', 128: 'territory', 129: 'government', 130: 'program', 131: '41', 132: 'wise', 133: 'services', 134: 'plant', 135: 'measures', 136: '01', 137: '2009', 138: '09m', 139: '9878m', 140: 'centre', 141: 'long', 142: 'sub', 143: 'responsible', 144: '136m', 145: 'stable', 146: '705m', 147: '121', 148: '220m', 149: 'meeting', 150: 'shaping', 151: '90', 152: '<START>', 153: 'buffer', 154: 'reducing', 155: 'superannuation', 156: '1857m', 157: '20392040', 158: '10040m', 159: 'maintain', 160: 'quamby', 161: 'minimise', 162: '915m', 163: 'property', 164: 'at', 165: '127', 166: 'full', 167: 'management', 168: 'credit', 169: 'risk', 170: '200506', 171: 'balanced', 172: 'child', 173: 'stromlo', 174: 'east', 175: 'over', 176: 'high'}\n"
     ]
    }
   ],
   "source": [
    "print(itl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622af7e",
   "metadata": {},
   "source": [
    "### Transformer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9614edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DModel = 512\n",
    "FfnHidden = 2048\n",
    "NumHeads = 8\n",
    "DropProb = 0.1\n",
    "NumLayers = 6\n",
    "MaxSequenceLength = 200\n",
    "VocabSize = len(vocab)\n",
    "StartToken ='<START>'\n",
    "EndToken ='<END>'\n",
    "PaddingToken ='<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18c266e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (Encoder): Encoder(\n",
       "    (SentenceEmbedding): SentenceEmbedding(\n",
       "      (Embedding): Embedding(177, 512)\n",
       "      (PositionEncoder): PE()\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (Layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (Norm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (Norm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (Norm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (Norm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (Norm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (Norm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (Norm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (Norm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (Norm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (Norm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (Attention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (Norm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (Norm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (SentenceEmbedding): SentenceEmbedding(\n",
       "      (Embedding): Embedding(177, 512)\n",
       "      (PositionEncoder): PE()\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (Layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (SelfAttention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (EncoderDecoderAttention): MultiHeadCrossAttention(\n",
       "          (KvLayer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (QLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (LayerNorm3): LayerNormalization()\n",
       "        (Dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (SelfAttention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (EncoderDecoderAttention): MultiHeadCrossAttention(\n",
       "          (KvLayer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (QLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (LayerNorm3): LayerNormalization()\n",
       "        (Dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (SelfAttention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (EncoderDecoderAttention): MultiHeadCrossAttention(\n",
       "          (KvLayer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (QLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (LayerNorm3): LayerNormalization()\n",
       "        (Dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (SelfAttention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (EncoderDecoderAttention): MultiHeadCrossAttention(\n",
       "          (KvLayer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (QLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (LayerNorm3): LayerNormalization()\n",
       "        (Dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (SelfAttention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (EncoderDecoderAttention): MultiHeadCrossAttention(\n",
       "          (KvLayer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (QLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (LayerNorm3): LayerNormalization()\n",
       "        (Dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (SelfAttention): MultiHeadAttention(\n",
       "          (QkvLayer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm1): LayerNormalization()\n",
       "        (Dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (EncoderDecoderAttention): MultiHeadCrossAttention(\n",
       "          (KvLayer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (QLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (LinearLayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (LayerNorm2): LayerNormalization()\n",
       "        (Dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (Ffn): PositionwiseFeedForward(\n",
       "          (Linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (Relu): ReLU()\n",
       "          (Dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (LayerNorm3): LayerNormalization()\n",
       "        (Dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Linear): Linear(in_features=512, out_features=177, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(DModel, FfnHidden, NumHeads, DropProb, NumLayers, MaxSequenceLength, VocabSize, lti, StartToken, EndToken, PaddingToken)\n",
    "model.to(getDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf901d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = CreateDataset(DatasetFilePath,lti,MaxSequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e347f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([152,  63,  63,  44,  55,  63,  77,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152,  44,  55,  24,  31,  17,  91, 175,  85,  23,  56,  51, 154,  14,\n",
      "        107, 113,  66,  95, 107,  27,   5,  43,  92, 129, 114,  76, 118,  63,\n",
      "         63,  63,  77,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  41,  63,  77,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152,  15,  34,   7, 135,   8, 147,  97,  57, 159, 106,  16, 168,  69,\n",
      "        116,  41,  18,  33, 126,  65,  26,  52, 161, 169, 176,  90, 133, 141,\n",
      "         57, 171,  55, 151, 155,   0, 101, 157, 105,  60,  41,  83, 111, 109,\n",
      "        125, 117, 169,   4,  75,  59,  83,  29,  63,  63,  63,  77,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  44,  18, 103,  77,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152,  18, 103, 173, 104,  64,  36,   7,  53,   5, 174,  10, 160,  81,\n",
      "         10,  20, 124,  61,  37, 140,  63,  63,  63,  77,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  44,  83, 111, 109,  77,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152,  83, 111, 109,  72,  71,   8,  89, 110, 107, 136,  63,  63,  63,\n",
      "         77,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,   4,  63,  77,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152,   4,  75,   8, 127, 131, 107,  82,  63,  63,  63,  77,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63, 155, 126,  77,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152, 155, 126,  58,   8, 165,  98,   5,  99, 107,  13,   5, 137,  63,\n",
      "         63,  63,  77,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  44, 102,  54,  15, 167,  77,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152, 102,  54, 143,  15, 167,   1, 126, 171,  74,  83,  48, 153, 119,\n",
      "         40, 145,  62, 166,  67,  63,  63,  63,  77,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  44,  65,  63,  77,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152, 159,  16, 171,  55, 175,  44,  63,  63,  63,  63,  63,  77,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  44, 133,  63,  77,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152,  46,  49,   6,  86,  94, 172,   7,  32, 122, 149,  93,  54, 100,\n",
      "          7, 123,   3, 121, 164,   2,  63,  63,  63,  77,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63])), (tensor([152,  63,  63,  44,  63,  63,  77,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]), tensor([152, 103,   5, 150,  50, 128,  28,  45, 115, 132, 130, 108,  42,  73,\n",
      "         25,  63,  63,  63,  77,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,  63,\n",
      "         63,  63,  63,  63]))]\n"
     ]
    }
   ],
   "source": [
    "print(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85c7b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to() received an invalid combination of arguments - got (function), but expected one of:\n * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlti\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEpochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mBatchSize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mLearningRate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mt:\\CodeBase\\Python\\jvai\\module\\train.py:17\u001b[39m, in \u001b[36mTrain\u001b[39m\u001b[34m(TransformerModel, Dataset, LanguageToIndex, Epochs, BatchSize, LearningRate)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m I \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(Dataset), BatchSize):\n\u001b[32m     16\u001b[39m     Batch = Dataset[I:I + BatchSize]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     Inputs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetDevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     Targets = torch.stack([X[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m Batch]).to(getDevice)\n\u001b[32m     20\u001b[39m     SeqLen = Inputs.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: to() received an invalid combination of arguments - got (function), but expected one of:\n * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n"
     ]
    }
   ],
   "source": [
    "trainModel(model, Dataset,lti,Epochs=10,BatchSize=2,LearningRate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestQueries = [\"What about debt?\",\"Tell me about infrastructure.\",\"What is the budget strategy?\"]\n",
    "TestResults = Test(model,lti ,itl, MaxSequenceLength, TestQueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Query, Response in TestResults:\n",
    "    print(f\"Query: {Query}\\nResponse: {Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccd774",
   "metadata": {},
   "source": [
    "## check q/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = Chatbot(Db,model,lti,itl,MaxSequenceLength,StartToken,EndToken,PaddingToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xc.Ask(\"What about debt?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c91103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xc.Ask(\"Tell me about infrastructure.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
